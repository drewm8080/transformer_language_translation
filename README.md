# Transformer-based Language Translation

## Description
This repository contains the implementation of a transformer-based model for language translation, with comparisons to RNN and LSTM models in terms of runtime and accuracy. It includes  self-attention mechanisms to handle the translation tasks effectively.

## Features
- Implementation of self attention layers.
- Positional encoding for input sequences.
- Comparison of model performance (runtime and accuracy) with RNN and LSTM models.
- Visualization of results

## Final Results





![image](https://github.com/drewm8080/transformer_language_translation/assets/71193439/6aac49d5-1112-46a9-9e1d-50c29c1ddac7)
